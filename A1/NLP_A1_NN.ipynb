{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "47d53905",
      "metadata": {
        "id": "47d53905"
      },
      "source": [
        "# Neural Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a497c4c6",
      "metadata": {
        "id": "a497c4c6"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d6808ffc",
      "metadata": {
        "id": "d6808ffc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gc\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading GloVe Dataset"
      ],
      "metadata": {
        "id": "nvMkSAziHRWm"
      },
      "id": "nvMkSAziHRWm"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0EIwVh966hj",
        "outputId": "f0230b39-0abd-41ba-c123-512a702d3d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-27 22:18:36--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-08-27 22:18:37--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-08-27 22:18:37--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.10MB/s    in 2m 39s  \n",
            "\n",
            "2022-08-27 22:21:17 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "id": "K0EIwVh966hj"
    },
    {
      "cell_type": "markdown",
      "id": "2c11f8f0",
      "metadata": {
        "id": "2c11f8f0"
      },
      "source": [
        "### Set Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5c94292a",
      "metadata": {
        "id": "5c94292a"
      },
      "outputs": [],
      "source": [
        "seq_len = 40\n",
        "num_epochs = 6\n",
        "batch_size = 25"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78cdcade",
      "metadata": {
        "id": "78cdcade"
      },
      "source": [
        "### Load_Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7e08167b",
      "metadata": {
        "id": "7e08167b"
      },
      "outputs": [],
      "source": [
        "def load_data() :\n",
        "    f_train = open(\"train.txt\",'r')\n",
        "    f_test = open(\"test.txt\",'r')\n",
        "    train_data  = [line.lower().strip() for line in f_train.readlines()]\n",
        "    test_data = [line.lower().strip() for line in f_test.readlines()]\n",
        "    print(\"No. of sentences in training data: \",len(train_data))\n",
        "    print(\"No. of sentences in test data: \",len(test_data))\n",
        "    return train_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e00c172d",
        "outputId": "3152bf95-7a65-4ae5-884a-255f57cfc36c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of sentences in training data:  60000\n",
            "No. of sentences in test data:  15000\n"
          ]
        }
      ],
      "source": [
        "train_data, test_data = load_data()"
      ],
      "id": "e00c172d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0f7f37f"
      },
      "source": [
        "### Process Training Data and Build the Model"
      ],
      "id": "f0f7f37f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4c0df8b"
      },
      "source": [
        "Create tokenizer on the training data"
      ],
      "id": "c4c0df8b"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fedc6db4",
        "outputId": "7f430787-c39b-47f9-b944-54ed448e1d6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size :  44691\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data)\n",
        "len_vocab = len(tokenizer.word_index) + 1;\n",
        "print(\"Vocabulary Size : \", len_vocab)"
      ],
      "id": "fedc6db4"
    },
    {
      "cell_type": "markdown",
      "id": "9919ce2b",
      "metadata": {
        "id": "9919ce2b"
      },
      "source": [
        "### Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "edee17e0",
      "metadata": {
        "id": "edee17e0"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(corpus, tokenizer) :\n",
        "    input_sequences = []\n",
        "    for line in corpus :\n",
        "      tokens = tokenizer.texts_to_sequences([line])[0]\n",
        "      input_sequences.append(tokens)\n",
        "\n",
        "    input_sequences = np.array(tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen = seq_len, padding = 'pre'))\n",
        "    train_input, train_output = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "    print(\"Predictors and Labels Summary :\")\n",
        "    print(input_sequences[:3])\n",
        "    return train_input, train_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "781cac06"
      },
      "source": [
        "Preprocess the training data to introduce padding and get uniform length Predictors"
      ],
      "id": "781cac06"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ba8d99af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee1bac1-6ff3-4659-cff2-b4fa663af94a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictors and Labels Summary :\n",
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0 4355  127 3069 2960  448  726  819]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0  109   43  326 4250    2   37 4768   44  101]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0 7636  831   19 1456   10    5  989   19  886  593]]\n"
          ]
        }
      ],
      "source": [
        "train_input, train_output = preprocess_data(train_data, tokenizer)"
      ],
      "id": "ba8d99af"
    },
    {
      "cell_type": "markdown",
      "id": "c7e497ee",
      "metadata": {
        "id": "c7e497ee"
      },
      "source": [
        "###Map words to Glove vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "aa587a48",
      "metadata": {
        "id": "aa587a48"
      },
      "outputs": [],
      "source": [
        "def create_glove_map(path) :\n",
        "    word_to_vec_map = {}\n",
        "    with open(path, 'r', encoding='UTF-8') as f:\n",
        "        for line in f:\n",
        "          w_line = line.split()\n",
        "          curr_word = w_line[0]\n",
        "          word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n",
        "    return word_to_vec_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dbdcab8"
      },
      "source": [
        "Use Glove to get pretrained word embeddings -"
      ],
      "id": "1dbdcab8"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "885fbde4",
        "outputId": "88e6d62e-9492-40e2-89e1-612846b1b4a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example: \n",
            " the   [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n"
          ]
        }
      ],
      "source": [
        "word_to_vec = create_glove_map(\"glove.6B.50d.txt\")\n",
        "print(\"Example: \\n\",\"the  \",word_to_vec['the'])"
      ],
      "id": "885fbde4"
    },
    {
      "cell_type": "markdown",
      "id": "286c9a24",
      "metadata": {
        "id": "286c9a24"
      },
      "source": [
        "### Create the Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5e41aa21",
      "metadata": {
        "id": "5e41aa21"
      },
      "outputs": [],
      "source": [
        "def get_embedding_layer(tokenizer, word_to_vec) :\n",
        "    words_to_index = tokenizer.word_index\n",
        "    len_vocab = len(words_to_index) + 1;\n",
        "    embed_vector_len = 50\n",
        "\n",
        "    emb_matrix = np.zeros((len_vocab, embed_vector_len))\n",
        "\n",
        "    for word, index in words_to_index.items():\n",
        "      embedding_vector = word_to_vec.get(word)\n",
        "      if embedding_vector is not None:\n",
        "        emb_matrix[index, :] = embedding_vector\n",
        "\n",
        "    print(\"Dimension of the embedding matrix: \\n\",emb_matrix.shape)\n",
        "\n",
        "    embedding_layer = tf.keras.layers.Embedding(input_dim = len_vocab, output_dim = embed_vector_len, input_length = seq_len - 1, weights = [emb_matrix], trainable = False)\n",
        "    return embedding_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3385d8f5"
      },
      "source": [
        "Create the Embedding Layer -  "
      ],
      "id": "3385d8f5"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5690681b",
        "outputId": "c597bb04-1c9c-43fd-c1f2-80335c3bd39f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension of the embedding matrix: \n",
            " (44691, 50)\n"
          ]
        }
      ],
      "source": [
        "embedding_layer = get_embedding_layer(tokenizer, word_to_vec)"
      ],
      "id": "5690681b"
    },
    {
      "cell_type": "markdown",
      "id": "9c340045",
      "metadata": {
        "id": "9c340045"
      },
      "source": [
        "### Create the Neural Language Model -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0ed0a9ab",
      "metadata": {
        "id": "0ed0a9ab"
      },
      "outputs": [],
      "source": [
        "def create_lstm_model(embedding_layer) :\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(embedding_layer)\n",
        "    model.add(tf.keras.layers.LSTM(1024))\n",
        "    model.add(tf.keras.layers.Dropout(0.1))\n",
        "    model.add(tf.keras.layers.Dense(len_vocab, activation = \"softmax\"))\n",
        "\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "    print(model.summary())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a9d55c1"
      },
      "source": [
        "Create the neural language model -"
      ],
      "id": "3a9d55c1"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff48998d",
        "outputId": "0c52a02f-7ba2-4e0c-f331-a7354ee1daff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 39, 50)            2234550   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 1024)              4403200   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 44691)             45808275  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 52,446,025\n",
            "Trainable params: 50,211,475\n",
            "Non-trainable params: 2,234,550\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "lstm_model = create_lstm_model(embedding_layer)"
      ],
      "id": "ff48998d"
    },
    {
      "cell_type": "markdown",
      "id": "3167a236",
      "metadata": {
        "id": "3167a236"
      },
      "source": [
        "### Function to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9be059d5",
      "metadata": {
        "id": "9be059d5"
      },
      "outputs": [],
      "source": [
        "def train_model(model, sample, data_frac) :\n",
        "    n = int((train_output.shape[0] * data_frac) / sample) * sample\n",
        "    print(\"Total Number of Samples :\", n // sample)\n",
        "\n",
        "    for i in range(0, num_epochs) :\n",
        "        print(\"Epoch : \", i + 1)\n",
        "        for j in range(0, n, sample) :\n",
        "            Y_train = tf.keras.utils.to_categorical(train_output[j : j + sample], len_vocab)\n",
        "            X_train = train_input[j : j + sample]\n",
        "            model.fit(X_train, Y_train, batch_size = batch_size, epochs = 1)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "292061b5"
      },
      "source": [
        "Training the Model"
      ],
      "id": "292061b5"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "f6fe03dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee8b7b46-fa86-4056-d87c-d3e7083edd03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Samples : 60\n",
            "Epoch :  1\n",
            "40/40 [==============================] - 7s 29ms/step - loss: 8.7470\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 7.5686\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 7.3571\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 7.1354\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 7.0008\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 6.6188\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 6.7591\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 6.5886\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 6.6990\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 6.3785\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 6.3744\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 6.3216\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 6.2905\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 6.1435\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 5.9486\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 5.8115\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 5.8671\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 5.9202\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 5.8813\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 5.8586\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 5.4749\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 5.6970\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 5.7215\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 5.4927\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 5.5990\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 5.3844\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 5.3398\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 5.5190\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 5.3972\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 5.5892\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 5.4000\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 5.1399\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 5.4200\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 5.2254\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 5.1245\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 5.1496\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 5.0638\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 5.1747\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 5.1594\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 5.0066\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 5.2895\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 5.1110\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 5.0067\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 5.0655\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.8470\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.9306\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 5.0930\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 4.8053\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 4.7132\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 4.8847\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 4.8328\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 4.7077\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 4.7640\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.6383\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.7854\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.6010\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 4.8647\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 4.5656\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.6660\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 4.7073\n",
            "Epoch :  2\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.1034\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 4.1925\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 4.1985\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.1451\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.2933\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 4.0809\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.2593\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 4.1803\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.3945\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 4.1685\n",
            "40/40 [==============================] - 1s 30ms/step - loss: 4.1692\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.2124\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.1635\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.1867\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.0810\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.9861\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.0470\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.0738\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.0090\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.0570\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.7352\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.9509\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 4.1137\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.8542\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.9269\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.7523\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.7252\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.8684\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 3.8121\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.9597\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 3.7785\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.6500\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.8483\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.5833\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.6372\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 3.5672\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 3.5988\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 3.6660\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.6984\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.4764\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.7659\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.7528\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.6540\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.5852\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.3756\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.6047\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.6060\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.3855\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.3528\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.4658\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.4240\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.2732\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.3242\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.3324\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.4539\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.2588\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.5011\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.2730\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.1553\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.2952\n",
            "Epoch :  3\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 3.0744\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 3.0467\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.0167\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.0015\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 3.1034\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 3.0000\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 3.1245\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.9378\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 3.1666\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.9639\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.9788\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 2.9897\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 2.9477\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.9693\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 2.8720\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 2.8862\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 2.8760\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.8954\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.7579\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 2.7660\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.5694\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.7283\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 2.8874\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 2.7354\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 2.6359\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.5203\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.4955\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.5908\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.5476\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 2.6527\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.4471\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 2.4442\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.5856\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.3741\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.3206\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.3186\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.3901\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.4548\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 2.3617\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.2229\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.5280\n",
            "40/40 [==============================] - 1s 30ms/step - loss: 2.5371\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.4144\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.2874\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 2.0993\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 2.3744\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.2128\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.1507\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.0833\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.1206\n",
            "40/40 [==============================] - 1s 30ms/step - loss: 2.1650\n",
            "40/40 [==============================] - 1s 30ms/step - loss: 2.0629\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 2.0674\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.0554\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.2011\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 2.0089\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 2.1951\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.9975\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.8609\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.9780\n",
            "Epoch :  4\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.9446\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.8641\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.8882\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.9740\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.9025\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.9848\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 2.0181\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.8390\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.9838\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.8482\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.8660\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.8975\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.7944\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.8123\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.8055\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.8021\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.7794\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.7997\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.6843\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.6804\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.6006\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.6511\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.7605\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.6716\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.4886\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.5084\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.5540\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.5362\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.5283\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.5643\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.4258\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.4459\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.5215\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.4289\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.4044\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.3575\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.3256\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.4519\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.4419\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.3094\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.4554\n",
            "40/40 [==============================] - 1s 30ms/step - loss: 1.4888\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.4596\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.3370\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.2755\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.4668\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.2785\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.2904\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.2188\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.2749\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.2371\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.1887\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.2303\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.2172\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.3495\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.1592\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.2750\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.0883\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.0925\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.0934\n",
            "Epoch :  5\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.1382\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.1818\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.2190\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.3013\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.2615\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.2272\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.2565\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.1547\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.2401\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.1460\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.1596\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.1880\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.1309\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.0816\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.0943\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.1320\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.0936\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.1360\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.9847\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.0224\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.0021\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 1.0362\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 1.1231\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.9932\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.8882\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.9626\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.9366\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.9950\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.9500\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.9498\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.9127\n",
            "40/40 [==============================] - 1s 30ms/step - loss: 0.9580\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.9775\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.9053\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.8365\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.8708\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.8235\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.9260\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.8524\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.8774\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.9489\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.9450\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.9413\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.8591\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.8516\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.8959\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.8272\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.8758\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.8219\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.8109\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.7927\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.7739\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.7575\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.7927\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.8947\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.7563\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.8767\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.7823\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.6964\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.7533\n",
            "Epoch :  6\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.7525\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.7831\n",
            "40/40 [==============================] - 1s 30ms/step - loss: 0.8328\n",
            "40/40 [==============================] - 1s 31ms/step - loss: 0.8853\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.8053\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.8155\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.8609\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.7642\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.7921\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.7673\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.7777\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.7876\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.7466\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.7372\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.7730\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.7637\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.7482\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.7345\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6991\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.7116\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.6713\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.7050\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.7590\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6483\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6184\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6814\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6689\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.7031\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6802\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6530\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6608\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6393\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6927\n",
            "40/40 [==============================] - 1s 30ms/step - loss: 0.6684\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.6433\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6376\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6217\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6104\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6314\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.5791\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6240\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6901\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6545\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.6122\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6207\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6670\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6232\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6261\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6381\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6669\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6221\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.5841\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6096\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6220\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6333\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6180\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.6335\n",
            "40/40 [==============================] - 1s 28ms/step - loss: 0.5396\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.5629\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.5503\n"
          ]
        }
      ],
      "source": [
        "trained_model = train_model(lstm_model, 1000, 1)"
      ],
      "id": "f6fe03dd"
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "filehandler = open(\"/content/drive/MyDrive/lstm_model.obj\",\"wb\")\n",
        "pickle.dump(trained_model,filehandler)\n",
        "filehandler.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95SCQwHhJJW-",
        "outputId": "08486c65-71dc-4974-8d1d-03ad1104662a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f4f400b0c90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ],
      "id": "95SCQwHhJJW-"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v_upWwxdit2",
        "outputId": "04c948ad-5392-4867-c8c5-96fb4ebc2bcf"
      },
      "id": "7v_upWwxdit2",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b5be0c6"
      },
      "source": [
        "### Function to evaluate the model"
      ],
      "id": "3b5be0c6"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5d73a142"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, Y_test, test_frac, sample) :\n",
        "    cross_entropy_loss = 0;\n",
        "    n = int(int((Y_test.shape[0]*test_frac)/sample)*sample)\n",
        "    for i in range(0,int(n),sample) :\n",
        "        Y = tf.keras.utils.to_categorical(Y_test[i:i+sample], len_vocab)\n",
        "        X = X_test[i:i+sample]\n",
        "        cross_entropy_loss += model.evaluate(X, Y, batch_size = batch_size)\n",
        "    return cross_entropy_loss/(n/sample)"
      ],
      "id": "5d73a142"
    },
    {
      "cell_type": "markdown",
      "id": "290ac258",
      "metadata": {
        "id": "290ac258"
      },
      "source": [
        "### Evaluating the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ac6f6beb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2339c1de-399f-433e-a934-2bb72bc85e42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictors and Labels Summary :\n",
            "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     1    39     6   296\n",
            "   2179    15   367   152    62     2 11940   144    57    19     7  1232\n",
            "     85     2  8651    10]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0  1889   305    30   595    37\n",
            "    684   221   120   306]\n",
            " [ 1400    85    15   371     3   145     3   153  2971     7  3368    44\n",
            "     23   154     2     1   400   513    14   168   747     5     1  5033\n",
            "      2  1245     5    77     3   245     1  1445    19   226    81     7\n",
            "   1780    95  4205     6]]\n"
          ]
        }
      ],
      "source": [
        "X_test, Y_test = preprocess_data(test_data, tokenizer)"
      ],
      "id": "ac6f6beb"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e169dc52",
      "metadata": {
        "id": "e169dc52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc72daed-0388-4364-899d-54a215374776"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 1s 12ms/step - loss: 5.0961\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 4.8525\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 4.7214\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 5.1893\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 4.8711\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 5.1142\n",
            "40/40 [==============================] - 0s 10ms/step - loss: 5.0108\n"
          ]
        }
      ],
      "source": [
        "cross_entropy_loss = evaluate_model(trained_model, X_test, Y_test, 0.5, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perplexity"
      ],
      "metadata": {
        "id": "uhf0YtUoZ0g-"
      },
      "id": "uhf0YtUoZ0g-"
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity = math.exp(cross_entropy_loss)\n",
        "print(\"Perplexity of the model on test set : \",perplexity)"
      ],
      "metadata": {
        "id": "MTdi06u_Z9R3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97157ef0-8c7e-4264-ba1a-5e71f8652312"
      },
      "id": "MTdi06u_Z9R3",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of the model on test set :  145.382105262822\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "name": "NLP_A1_NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}